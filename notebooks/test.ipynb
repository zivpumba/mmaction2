{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22ae3aa-1bf4-4e03-909d-88d6bc4c61b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/openmmlab/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m init_recognizer(config_file, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# inference the demo video\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[43minference_recognizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../demo/demo.mp4\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mmaction2/mmaction/apis/inference.py:172\u001b[0m, in \u001b[0;36minference_recognizer\u001b[0;34m(model, video, outputs, as_tensor, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    166\u001b[0m         audio_path\u001b[38;5;241m=\u001b[39mvideo,\n\u001b[1;32m    167\u001b[0m         total_frames\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mload(video)),\n\u001b[1;32m    168\u001b[0m         start_index\u001b[38;5;241m=\u001b[39mcfg\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mtest\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m),\n\u001b[1;32m    169\u001b[0m         label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    171\u001b[0m test_pipeline \u001b[38;5;241m=\u001b[39m Compose(test_pipeline)\n\u001b[0;32m--> 172\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtest_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m data \u001b[38;5;241m=\u001b[39m collate([data], samples_per_gpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mis_cuda:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# scatter to specified GPU\u001b[39;00m\n",
      "File \u001b[0;32m~/mmaction2/mmaction/datasets/pipelines/compose.py:50\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;124;03m\"\"\"Call function to apply transforms sequentially.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    dict: Transformed data.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 50\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mmaction2/mmaction/datasets/pipelines/loading.py:231\u001b[0m, in \u001b[0;36mSampleFrames.__call__\u001b[0;34m(self, results)\u001b[0m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m\"\"\"Perform the SampleFrames loading.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \n\u001b[1;32m    225\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;124;03m    results (dict): The resulting dict to be modified and passed\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m        to the next transform in pipeline.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m total_frames \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_frames\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 231\u001b[0m clip_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample_clips\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    232\u001b[0m frame_inds \u001b[38;5;241m=\u001b[39m clip_offsets[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_len)[\u001b[38;5;28;01mNone\u001b[39;00m, :] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_interval\n\u001b[1;32m    234\u001b[0m frame_inds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(frame_inds)\n",
      "File \u001b[0;32m~/mmaction2/mmaction/datasets/pipelines/loading.py:216\u001b[0m, in \u001b[0;36mSampleFrames._sample_clips\u001b[0;34m(self, num_frames)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m\"\"\"Choose clip offsets for the video in a given mode.\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m    np.ndarray: Sampled frame indices.\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_mode:\n\u001b[0;32m--> 216\u001b[0m     clip_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_test_clips\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     clip_offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_clips(num_frames)\n",
      "File \u001b[0;32m~/mmaction2/mmaction/datasets/pipelines/loading.py:199\u001b[0m, in \u001b[0;36mSampleFrames._get_test_clips\u001b[0;34m(self, num_frames)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_frames \u001b[38;5;241m>\u001b[39m ori_clip_len \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    198\u001b[0m     base_offsets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_clips) \u001b[38;5;241m*\u001b[39m avg_interval\n\u001b[0;32m--> 199\u001b[0m     clip_offsets \u001b[38;5;241m=\u001b[39m (base_offsets \u001b[38;5;241m+\u001b[39m avg_interval \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint\u001b[49m)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtwice_sample:\n\u001b[1;32m    201\u001b[0m         clip_offsets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([clip_offsets, base_offsets])\n",
      "File \u001b[0;32m/opt/conda/envs/openmmlab/lib/python3.8/site-packages/numpy/__init__.py:284\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tester\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Tester\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;18m__name__\u001b[39m, attr))\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mmaction.apis import init_recognizer, inference_recognizer\n",
    "\n",
    "config_file = '../configs/recognition/tsn/tsn_r50_video_inference_1x1x3_100e_kinetics400_rgb.py'\n",
    "device = 'cuda:0' # or 'cpu'\n",
    "device = torch.device(device)\n",
    "\n",
    "model = init_recognizer(config_file, device=device)\n",
    "# inference the demo video\n",
    "inference_recognizer(model, '../demo/demo.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb2fd23-0a06-4195-8d9d-c04a343ab924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openmmlab",
   "language": "python",
   "name": "openmmlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
